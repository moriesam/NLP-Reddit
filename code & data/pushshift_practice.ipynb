{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives & Requirements \n",
    "- Gather and prepare your data using the requests library.\n",
    "- Create and compare two models. One of these must be a Bayes classifier, however the other can be a - classifier of your choosing: logistic regression, KNN, SVM, etc.\n",
    "- A Jupyter Notebook with your analysis for a peer audience of data scientists.\n",
    "- An executive summary of your results.\n",
    "- A short presentation outlining your process and findings for a semi-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Requests and Beautiful Soup to Extract Information From a Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import requests, json\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import time \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from sklearn.datasets import make_classification\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling content by url and entering into a dataframe : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining different timestamps so we can loop through and pull enough data (9K) for each target subject\n",
    "time_stamps = ['2019-12-28 15:07:53',\n",
    "'2019-10-17 11:07:49',\n",
    "'2019-08-26 19:11:01',\n",
    "'2019-06-28 20:48:35',\n",
    "'2019-04-13 23:00:50',\n",
    "'2018-12-30 17:36:15',\n",
    "'2018-09-03 18:17:58',\n",
    "'2017-12-21 14:29:13',\n",
    "'2017-02-16 16:40:38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating loop to pull data:\n",
    "\n",
    "# set up list \n",
    "data_gw = []\n",
    "data_cs = []\n",
    "for time in time_stamps: \n",
    "    #text ={}\n",
    "# select and target web page:    \n",
    "    url = 'https://api.pushshift.io/reddit/submission/search'\n",
    "#setting up parameters\n",
    "    params_gw = {'subreddit': 'globalwarming', 'size': 1000, 'before':time}\n",
    "    params_cs = {'subreddit': 'climateskeptics', 'size': 1000, 'before':time}\n",
    "# Establishing the connection to the web page:\n",
    "    res_gw = requests.get(url, params_gw)\n",
    "    res_cs = requests.get(url, params_cs)\n",
    "#check status codes to see if target server responds well - successful status code is confirmed.\n",
    "    #print(res_gw.status_code)\n",
    "    #print(res_cs.status_code)\n",
    "# #Create two sets of dataframe for target subjects, converting requests to json and add to the list. \n",
    "    text_gw = pd.DataFrame(res_gw.json()['data'])\n",
    "    text_cs = pd.DataFrame(res_cs.json()['data'])\n",
    "    data_gw.append(text_gw)\n",
    "    data_cs.append(text_cs)\n",
    "    \n",
    "#define \n",
    "\n",
    "    #for time in time_stamps:\n",
    "        #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two sets of dataframe for target subjects, concatenate 2 lists and dataframe \n",
    "frames = data_gw + data_cs\n",
    "df = pd.concat(frames, axis = 0, sort=False)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_gw[0]['score'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#data_cs[0]['score'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging title and texts into one field for richer content\n",
    "df[\"title_selftext\"] = df[\"title\"] + \" \" + df[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Does big reports og forums exist with analysis...\n",
       "1        It was 66 here in indiana on christmas day I a...\n",
       "2        HELP, AUSTRALIA IS DYING AND NO ONE IS TALKING...\n",
       "3        Global warming is BULLSHIT! I hate all this fa...\n",
       "4        More Bullshit!!! \"European cities will be sunk...\n",
       "                               ...                        \n",
       "17995           France to drop carbon tax plan: Les Echos \n",
       "17996                               The EPA Caves on Coal \n",
       "17997    California targets dairy cow farts to combat g...\n",
       "17998    Why Climate Change Deniers Should Still Act On...\n",
       "17999    UCS -stands for the Union of Con Scientists. L...\n",
       "Name: title_selftext, Length: 17937, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing non/blank values to remove data noise \n",
    "df['title_selftext'].dropna(axis = 0, inplace=True)\n",
    "df['title_selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does big reports og forums exist with analysis...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was 66 here in indiana on christmas day I a...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HELP, AUSTRALIA IS DYING AND NO ONE IS TALKING...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global warming is BULLSHIT! I hate all this fa...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>More Bullshit!!! \"European cities will be sunk...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_selftext      subreddit\n",
       "0  Does big reports og forums exist with analysis...  GlobalWarming\n",
       "1  It was 66 here in indiana on christmas day I a...  GlobalWarming\n",
       "2  HELP, AUSTRALIA IS DYING AND NO ONE IS TALKING...  GlobalWarming\n",
       "3  Global warming is BULLSHIT! I hate all this fa...  GlobalWarming\n",
       "4  More Bullshit!!! \"European cities will be sunk...  GlobalWarming"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking into dataframe to check the result\n",
    "df = df[['title_selftext', 'subreddit']]\n",
    "df['title_selftext'] = df['title_selftext'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieved from Dan Kim feature_union notebook\n",
    "# instantiate the sentiment analyzer \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "# Write a function to get the compound sentiment scores for a post\n",
    "def get_compound_sentiment(post):\n",
    "    return sia.polarity_scores(post)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and add sentiment column to the dataframe \n",
    "df['sentiment'] = df['title_selftext'].apply(lambda x: get_compound_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the distribution of the composite sentiment score - Retrieved from Dan Kim feature_union notebook\n",
    "df.groupby('subreddit')['sentiment'].plot(kind = 'hist', legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global warming is BULLSHIT! I hate all this fa...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "      <td>-0.9956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>More Bullshit!!! \"European cities will be sunk...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "      <td>-0.8777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A vet's hellish impressions of climate change</td>\n",
       "      <td>GlobalWarming</td>\n",
       "      <td>-0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Am I… Am I really going to die at 47? 2050 is ...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "      <td>-0.7916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is worse for the planet? Carbon emissions...</td>\n",
       "      <td>GlobalWarming</td>\n",
       "      <td>-0.5362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17968</th>\n",
       "      <td>The climate scam corruption metastasizes</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>-0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17970</th>\n",
       "      <td>Two-thirds of Australians think reef crisis is...</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>-0.7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17981</th>\n",
       "      <td>Rampant climate fraud makes me doubt other hig...</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>-0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17983</th>\n",
       "      <td>NASA Drops Global Warming BOMBSHELL, Liberals ...</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>-0.7717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17988</th>\n",
       "      <td>Record cold coming to ‘almost entire USA’ – Lo...</td>\n",
       "      <td>climateskeptics</td>\n",
       "      <td>-0.7125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2462 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title_selftext        subreddit  \\\n",
       "3      Global warming is BULLSHIT! I hate all this fa...    GlobalWarming   \n",
       "4      More Bullshit!!! \"European cities will be sunk...    GlobalWarming   \n",
       "5         A vet's hellish impressions of climate change     GlobalWarming   \n",
       "6      Am I… Am I really going to die at 47? 2050 is ...    GlobalWarming   \n",
       "12     What is worse for the planet? Carbon emissions...    GlobalWarming   \n",
       "...                                                  ...              ...   \n",
       "17968          The climate scam corruption metastasizes   climateskeptics   \n",
       "17970  Two-thirds of Australians think reef crisis is...  climateskeptics   \n",
       "17981  Rampant climate fraud makes me doubt other hig...  climateskeptics   \n",
       "17983  NASA Drops Global Warming BOMBSHELL, Liberals ...  climateskeptics   \n",
       "17988  Record cold coming to ‘almost entire USA’ – Lo...  climateskeptics   \n",
       "\n",
       "       sentiment  \n",
       "3        -0.9956  \n",
       "4        -0.8777  \n",
       "5        -0.5106  \n",
       "6        -0.7916  \n",
       "12       -0.5362  \n",
       "...          ...  \n",
       "17968    -0.5719  \n",
       "17970    -0.7901  \n",
       "17981    -0.9095  \n",
       "17983    -0.7717  \n",
       "17988    -0.7125  \n",
       "\n",
       "[2462 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deep dive into negative posts for better undrestanding of what is driving it \n",
    "df[df['sentiment']<-0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "climateskeptics    9000\n",
       "GlobalWarming      9000\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if all values are collected - all looks good!\n",
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-processing | Modeling | Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process has been used to verify timestamp for data pull.\n",
    "#df = df[['created_utc', 'retrieved_on','title_selftext', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing non/blank values to remove data noise \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting subreddit into binary values: \n",
    "\n",
    "> - 0 for GlobalWarming\n",
    "> - 1 for climateskeptics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9000\n",
       "0    9000\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'] = np.where(df['subreddit']=='climateskeptics', 1,0)\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning  | Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing urls\n",
    "df['title_selftext'] = df['title_selftext'].map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*', ' ', x)) # removing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all punctuation\n",
    "df['title_selftext'] = df['title_selftext'].map(lambda x: re.sub('[^\\w\\s]', ' ', x)) # Removing all punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keeping letters \n",
    "df['title_selftext'] = df['title_selftext'].map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x)) # Only keeping letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindexing df to be able to merge column into new words_df dataframe\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using countervectorizer to tokenize our texts into collection of words \n",
    "words = df[\"title_selftext\"]\n",
    "#Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Run lemmatizer.\n",
    "tokens_lem =  [lemmatizer.lemmatize(i) for i in words]\n",
    "# removing stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "filtered_words = [word for word in tokens_lem if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using countervectorizer to tokenize our texts into collection of words \n",
    "cvec = CountVectorizer(min_df =10)\n",
    "words_matrix = cvec.fit_transform(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a datframe from all tokenized words  \n",
    "words_df = pd.DataFrame(words_matrix.toarray(), columns=cvec.get_feature_names())\n",
    "words_df['sentiment'] = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'of', 'and', 'is', 'in', 'climate', 'that', 'it', 'global', 'warming', 'this', 'change', 'for', 'on', 'are', 'we', 'be', 'you', 'not', 'as', 'have', 'with', 'by', 'about', 'what', 'from', 'but', 'if', 'can', 'will', 'all', 'so', 'they', 'how', 'or', 'more', 'at', 'do', 'an', 'was', 'co', 'just', 'would', 'my', 'there', 'people', 'has', 'world', 'earth', 'years', 'our', 'removed', 'like', 'out', 'why', 'carbon', 'up', 'than', 'new', 'some', 'no', 'their', 'ice', 'science', 'one', 'could', 'temperature', 'now', 'me', 'don', 'us', 'scientists', 'time', 'which', 'been', 'because', 'any', 'over', 'your', 'much', 'planet', 'think', 'when', 'who', 'know', 'into', 'data', 'energy', 'here', 'year', 'only', 'these', 'even', 'amp', 'get', 'should', 'made', 'see', 'sea', 'water', 'make', 'most', 'help', 'solar', 'other', 'also', 'going', 'real', 'its', 'human', 'then', 'he', 'really', 'does', 'many', 'were', 'emissions', 'need', 'use', 'very', 'being', 'want', 'them', 'way', 'believe', 'am', 'green', 'heat', 'where', 'stop', 'say', 'good', 're', 'greenhouse', 'since', 'trump', 'rise', 'may', 'those', 'first', 'record', 'atmosphere', 'level', 'weather', 'right', 'study', 'ocean', 'too', 'man']\n"
     ]
    }
   ],
   "source": [
    "#looking into top words by each class\n",
    "top_words_gw = list(words_df.groupby('target').\n",
    "     mean().T.sort_values(0, ascending=False).head(150).index)\n",
    "\n",
    "top_words_cs = list(words_df.groupby('target').\n",
    "     mean().T.sort_values(1, ascending=False).head(150).index)\n",
    "\n",
    "print(top_words_gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'and', 'of', 'we', 'that', 'in', 'they', 'it', 'is', 'was', 'this', 'not', 'by', 'she', 'but', 'on', 'for', 'emissions', 'so', 'with', 'can', 'as', 'their', 'her', 'time', 'should', 'climate', 'more', 'if', 'just', 'are', 'like', 'or', 'many', 'about', 'how', 'where', 'because', 'still', 'targets', 'from', 'be', 'will', 'australia', 'here', 'see', 'people', 'tax', 'carbon', 'would', 'had', 'energy', 'our', 'up', 'power', 'other', 'any', 'big', 'government', 'don', 'do', 'problems', 'what', 'levels', 'me', 'historical', 'change', 'care', 'new', 'didn', 'attacked', 'even', 'my', 'said', 'you', 'know', 'strategy', 'being', 'conservative', 'ghg', 'over', 'set', 'nuclear', 'education', 'have', 'pay', 'past', 'reduce', 'believe', 'than', 'methane', 'water', 'were', 'reduction', 'idea', 'atmosphere', 'chemical', 'point', 'greenhouse', 'pollution', 'hold', 'gases', 'suggestions', 'suggestion', 'out', 'he', 'at', 'ice', 'election', 'calling', 'am', 'business', 'an', 'each', 'think', 'countries', 'such', 'into', 'towards', 'minister', 'answer', 'now', 'plan', 'get', 'best', 'anyone', 'god', 'heart', 'prime', 'australian', 'fossil', 'human', 'changes', 'paid', 'taxes', 'something', 'changed', 'fellow', 'come', 'own', 'price', 'sources', 'run', 'cars', 'way', 'hoax', 'going', 'both', 'need']\n"
     ]
    }
   ],
   "source": [
    "print(top_words_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up our data for modeling. X is all words column from the new df, Y is the subject label \n",
    "X = words_df\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model\n",
    "logreg =LogisticRegression(solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up parameters \n",
    "params_logreg = {'penalty': ['l1', 'l2']} # we use penalty to diffrentiate vs lasso and ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None, param_grid={'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gridsearch to loop thorough the parameters and finds the best\n",
    "gs_logreg = GridSearchCV(logreg, params_logreg, cv = 5)\n",
    "gs_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8233333333333334\n",
      "accuracy score - Existing data 0.9068888888888889\n",
      "accuracy score - New data 0.8342222222222222\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "# print out the best score\n",
    "print(gs_logreg.best_score_)\n",
    "#print out the set of hyperparameters that achieved the best score.\n",
    "logreg_model = gs_logreg.best_estimator_\n",
    "print('accuracy score - Existing data', logreg_model.score(X_train, y_train))\n",
    "print('accuracy score - New data',  logreg_model.score(X_test, y_test)) # Accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look int coefficients and put it into a dataframe\n",
    "features = X\n",
    "features = list(features)\n",
    "coefs = gs_logreg.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0.465461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abc</td>\n",
       "      <td>0.220815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability</td>\n",
       "      <td>0.065879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>able</td>\n",
       "      <td>-0.364832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>about</td>\n",
       "      <td>0.144341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.197319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>zharkova</td>\n",
       "      <td>0.481640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>zone</td>\n",
       "      <td>-0.325130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>zones</td>\n",
       "      <td>0.382683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>sentiment</td>\n",
       "      <td>-0.632404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4272 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       features  coefficients\n",
       "0       abandon      0.465461\n",
       "1           abc      0.220815\n",
       "2       ability      0.065879\n",
       "3          able     -0.364832\n",
       "4         about      0.144341\n",
       "...         ...           ...\n",
       "4267       zero      0.197319\n",
       "4268   zharkova      0.481640\n",
       "4269       zone     -0.325130\n",
       "4270      zones      0.382683\n",
       "4271  sentiment     -0.632404\n",
       "\n",
       "[4272 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_df = pd.DataFrame({'features': features, 'coefficients': coefs[0]})\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 -  Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'max_depth': [None, 10], 'n_estimators': [100, 150]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup paramaters & Gridsearch \n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 10],\n",
    "}\n",
    "gs_rf = GridSearchCV(rf, param_grid=rf_params, cv=5)\n",
    "gs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8879259259259259\n",
      "accuracy score - Existing data 0.9935555555555555\n",
      "accuracy score - New data 0.9004444444444445\n"
     ]
    }
   ],
   "source": [
    "# print out the best score\n",
    "print(gs_rf.best_score_)\n",
    "#print out the set of hyperparameters that achieved the best score.\n",
    "rf_model = gs_rf.best_estimator_\n",
    "print('accuracy score - Existing data', rf_model.score(X_train, y_train))\n",
    "print('accuracy score - New data', rf_model.score(X_test, y_test)) # Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model took longer time than anticipated for large set of data, thus I wont be tuning it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ada_params = {\n",
    "#    'n_estimators': [100,150],\n",
    "#    'base_estimator__max_depth': [1,2],\n",
    "#    'learning_rate': [.9, 1.]\n",
    "#}\n",
    "#gs_ada = GridSearchCV(ada, param_grid=ada_params, cv=3)\n",
    "#gs_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out the best score\n",
    "#print(gs_ada.best_score_)\n",
    "##print out the set of hyperparameters that achieved the best score.\n",
    "#ada_model = gs_ada.best_estimator_\n",
    "#print('accuracy score - Existing data', ada_model.score(X_train, y_train))\n",
    "#print('accuracy score - New data', ada_model.score(X_test, y_test)) # Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Gradient Boosting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters and Gridsearch \n",
    "gb_params = {\n",
    "   'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 150],\n",
    "    'learning_rate': [.08, .1, .12],\n",
    "    #'max_features': [100]\n",
    "}\n",
    "gs_gb = GridSearchCV(gb, param_grid=gb_params, cv=3)\n",
    "gs_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# print out the best score\n",
    "print(gs_gb.best_score_)\n",
    "#print out the set of hyperparameters that achieved the best score.\n",
    "gb_model = gs_gb.best_estimator_\n",
    "print('accuracy score - Existing data', gb_model.score(X_train, y_train))\n",
    "print('accuracy score - New data', gb_model.score(X_test, y_test)) # Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - Voting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sake of time & speed no parameters were run for the votingclassifier\n",
    "# but will keep/run the code for future references. \n",
    "\n",
    "##using GridSearch now to loop through multiple parameter and check which one works the best for our model:\n",
    "## Fit GridSearch to train set.\n",
    "#gs = GridSearchCV(vote, param_grid=vote_params, cv=3)\n",
    "## Fit GridSearch to train set.\n",
    "#gs.fit(X_train, y_train)\n",
    "## print out the best score\n",
    "#print(gs.best_score_)\n",
    "##print out the set of hyperparameters that achieved the best score.\n",
    "#print(gs.best_params_)\n",
    "#vote_model = gs.best_estimator_\n",
    "#print(vote_model.score(X_train, y_train))\n",
    "#print(vote_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vote = VotingClassifier([\n",
    "#    ('gb', GradientBoostingClassifier()), \n",
    "#    ('lr', LogisticRegression(solver = 'liblinear')),\n",
    "#    ('rf', RandomForestClassifier())])\n",
    "#\n",
    "## setting up params n_estimators is set based on squareroot number of 20K rows ~150\n",
    "##vote_params = {\n",
    "##    'gb__n_estimators': [100,150],\n",
    "##    'lr__n_estimators': [100,150],\n",
    "##    'rf__max_depth': [None, 10],\n",
    "#    \n",
    "##}\n",
    "##using GridSearch now to loop through multiple parameter and check which one works the best for our model:\n",
    "## Fit GridSearch to train set.\n",
    "#gs = GridSearchCV(vote, param_grid=vote_params, cv=3)\n",
    "## Fit GridSearch to train set.\n",
    "#gs.fit(X_train, y_train)\n",
    "## print out the best score\n",
    "#print(gs.best_score_)\n",
    "##print out the set of hyperparameters that achieved the best score.\n",
    "#print(gs.best_params_)\n",
    "#vote_model = gs.best_estimator_\n",
    "#print(vote_model.score(X_train, y_train))\n",
    "#print(vote_model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
